<code class='bash'>
dan@a78:~ $ 
dan@a78:~ $ cd ~/spark
dan@a78:~/spark $



dan@a78:~/spark $ bin/spark-shell
Spark context Web UI available at http://192.168.1.78:4040
Spark context available as 'sc' (master = local[*], app id = local-1498175577126).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_131)
Type in expressions to have them evaluated.
Type :help for more information.

scala>



scala> val textFile = sc.textFile("README.md")
textFile: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at &lt;console>:24
scala> 



scala> textFile.count()
res0: Long = 104



scala> textFile.first()
res1: String = # Apache Spark



scala> val linesWithSpark = textFile.filter(line => line.contains("Spark"))
linesWithSpark: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at &lt;console>:26



scala> textFile.filter(line => line.contains("Spark")).count()
res2: Long = 20



scala> textFile.map(line => line.split(" ").size).reduce((a, b) => if (a > b) a else b)
res3: Int = 22



scala> import java.lang.Math
import java.lang.Math



scala> textFile.map(line => line.split(" ").size).reduce((a, b) => Math.max(a, b))
res4: Int = 22



scala> val wordCounts = textFile.flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey((a, b) => a + b)
wordCounts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[10] at reduceByKey at &lt;console>:27



scala> wordCounts.collect()
res5: Array[(String, Int)] = Array((package,1), (this,1),
(Version"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version),1),
(Because,1), (Python,2),
(page](http://spark.apache.org/documentation.html).,1), (cluster.,1),
(its,1), ([run,1), (general,3), (have,1), (pre-built,1), (YARN,,1),
([http://spark.apache.org/developer-tools.html](the,1), (changed,1),
(locally,2), (sc.parallelize(1,1), (only,1), (locally.,1),
(several,1), (This,2), (basic,1), (Configuration,1), (learning,,1),
(documentation,3), (first,1), (graph,1), (Hive,2), (info,1),
(["Specifying,1), ("yarn",1), ([params]`.,1), ([project,1),
(prefer,1), (SparkPi,2), (&lt;http://spark.apache.org/>,1), (engine,1),
(version,1), (file,1), (documentation,,1), (MASTER,1), (example,3),
(["Parallel,1), (are...



scala> linesWithSpark.cache()
res6: linesWithSpark.type = MapPartitionsRDD[4] at filter at &lt;console>:26



scala> linesWithSpark.count()
res7: Long = 20



scala> linesWithSpark.count()
res8: Long = 20
</code>
