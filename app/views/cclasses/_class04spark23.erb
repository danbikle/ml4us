<code class='bash'>
dan@a78:~/ml4/public/class04 $ ~/spark/bin/spark-shell -i ml_examp1.scala
Spark context Web UI available at http://192.168.1.78:4040
Spark context available as 'sc' (master = local[*], app id = local-1498246691356).
Spark session available as 'spark'.
Loading ml_examp1.scala...
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.sql.Row
training: org.apache.spark.sql.DataFrame = [label: double, features: vector]
lr: org.apache.spark.ml.classification.LogisticRegression = logreg_888c36ee4594
LogisticRegression parameters:
aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)
elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)
family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial. (default: auto)
featuresCol: features column name (default: features)
fitIntercept: whether to fit an intercept term (default: true)
labelCol: label column name (default: label)
maxIter: maximum number of iterations (>= 0) (default: 100)
predictionCol: prediction column name (default: prediction)
probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)
rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)
regParam: regularization parameter (>= 0) (default: 0.0)
standardization: whether to standardize the training features before fitting the model (default: true)
threshold: threshold in binary classification prediction, in range [0, 1] (default: 0.5)
thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)
tol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-6)
weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)

res1: lr.type = logreg_888c36ee4594
model1: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_888c36ee4594
Model 1 was fit using parameters: {
	logreg_888c36ee4594-aggregationDepth: 2,
	logreg_888c36ee4594-elasticNetParam: 0.0,
	logreg_888c36ee4594-family: auto,
	logreg_888c36ee4594-featuresCol: features,
	logreg_888c36ee4594-fitIntercept: true,
	logreg_888c36ee4594-labelCol: label,
	logreg_888c36ee4594-maxIter: 10,
	logreg_888c36ee4594-predictionCol: prediction,
	logreg_888c36ee4594-probabilityCol: probability,
	logreg_888c36ee4594-rawPredictionCol: rawPrediction,
	logreg_888c36ee4594-regParam: 0.01,
	logreg_888c36ee4594-standardization: true,
	logreg_888c36ee4594-threshold: 0.5,
	logreg_888c36ee4594-tol: 1.0E-6
}
paramMap: org.apache.spark.ml.param.ParamMap =
{
	logreg_888c36ee4594-maxIter: 30,
	logreg_888c36ee4594-regParam: 0.1,
	logreg_888c36ee4594-threshold: 0.55
}
paramMap2: org.apache.spark.ml.param.ParamMap =
{
	logreg_888c36ee4594-probabilityCol: myProbability
}
paramMapCombined: org.apache.spark.ml.param.ParamMap =
{
	logreg_888c36ee4594-maxIter: 30,
	logreg_888c36ee4594-probabilityCol: myProbability,
	logreg_888c36ee4594-regParam: 0.1,
	logreg_888c36ee4594-threshold: 0.55
}
model2: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_888c36ee4594
Model 2 was fit using parameters: {
	logreg_888c36ee4594-aggregationDepth: 2,
	logreg_888c36ee4594-elasticNetParam: 0.0,
	logreg_888c36ee4594-family: auto,
	logreg_888c36ee4594-featuresCol: features,
	logreg_888c36ee4594-fitIntercept: true,
	logreg_888c36ee4594-labelCol: label,
	logreg_888c36ee4594-maxIter: 30,
	logreg_888c36ee4594-predictionCol: prediction,
	logreg_888c36ee4594-probabilityCol: myProbability,
	logreg_888c36ee4594-rawPredictionCol: rawPrediction,
	logreg_888c36ee4594-regParam: 0.1,
	logreg_888c36ee4594-standardization: true,
	logreg_888c36ee4594-threshold: 0.55,
	logreg_888c36ee4594-tol: 1.0E-6
}
test: org.apache.spark.sql.DataFrame = [label: double, features: vector]
([-1.0,1.5,1.3], 1.0) -> prob=[0.05707304171033979,0.9429269582896603], prediction=1.0
([3.0,2.0,-0.1], 0.0) -> prob=[0.923852231170409,0.076147768829591], prediction=0.0
([0.0,2.2,-1.5], 1.0) -> prob=[0.10972776114779162,0.8902722388522084], prediction=1.0

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_131)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 
scala> 
scala> 
</code>
